{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Попробуйте изменить параметры нейронной сети работающей с датасетом imdb так, чтобы улучшить ее точность. Приложите анализ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я сильно изменял каждый параметр и смотрел, что изменилось при обучении, затем возвращал в изначальное положение и пробовал снова, вот результат этого небольшого анализа? <br>\n",
    "1) Проверим гипотезу о том, что по первым словам можно понять положительный это отзыв или отрицательный. Я изменил максимальную длину, уменьшил её до 50, обучаться стало в полтора раза быстрее, но качество упало на 1.5%.<br>\n",
    "2) Батч сайз влияет всё так же - чем он меньше, тем дольше обучается модель, но тем лучше результат. Уменьшение до 25 дало прирост точности до 0.8276.<br>\n",
    "3) Cлои Embedding имеет 2 параметра - размер словаря (max_features) и размер векторного представления. Я предполагаю, что увеличение размерности векторного представления увеличит смысловую нагрузку, которые эти векторы смогут нести, т.е. слова станут более осмысленные для модели. Однако, хоть модель и обучалась гораздо дольше, качество у меня упало. Так же проверим, что размер словаря может быть небольшим, т.к. нам не надо большинство слов-связок. Я понизил max_features в 1- раз и качество снизилось всего на 1%.<br>\n",
    "4) Увеличение размера LSTM ячейки где-то в 2 раза увеличило время обучения где-то в 3 раза, при этом качество увеличилось на тестовом прогоне до 0.8275.<br>\n",
    "5) Дропаут в LSTM помогает избежать переобучения, но высокое значение может помешать в принципе обучить модель. При дропауте в 0.5 ничего особо не изменилось, ни точность, ни скорость. А вот такое же качество recurrent_dropout увеличило точность на 1%.<br>\n",
    "6) В последнем слое я поменял функцию активации с sigmoid на relu. В результате точность упала на 4%. Полагаю, что вид sigmoid (плавный от 0 до 1) больше подходит для использовании в выходном слое.<br>\n",
    "7) Оптимайзер SGD быстрее сработал, чем adam, но выдал результат около 0.5. Полагаю, что он не успел даже начать двигаться в нужном направлении, возможно, надо увеличить число эпох."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Попробуйте изменить параметры нейронной сети генерирующий текст таким образом, чтобы добиться генерации как можно более осмысленного текста. Пришлите лучший получившейся у вас текст и опишите, что вы предприняли, чтобы его получить. Можно использовать текст другого прозведения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Итерация #: 0\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 64s 405us/step - loss: 2.3746\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 64s 404us/step - loss: 1.9050\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 62s 392us/step - loss: 1.7025\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 63s 398us/step - loss: 1.5612\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 66s 413us/step - loss: 1.45270s - loss: 1.45 - ETA: 0s - l\n",
      "Генерация из посева: bbing of the\n",
      "bbing of the court, and she was so they was a little she was a great did all the thing was the door of the court==================================================\n",
      "Итерация #: 1\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 66s 418us/step - loss: 1.3676\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 66s 413us/step - loss: 1.2941\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 65s 412us/step - loss: 1.2305\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 65s 407us/step - loss: 1.1727\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 64s 406us/step - loss: 1.1194\n",
      "Генерация из посева: ties or the \n",
      "ties or the eldes of the mock turtle said to herself in a sore of the lows of the look of the lows of the look o==================================================\n",
      "Итерация #: 2\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 64s 405us/step - loss: 1.0682\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 67s 419us/step - loss: 1.0206\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 67s 423us/step - loss: 0.9723\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 68s 427us/step - loss: 0.9261\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 67s 424us/step - loss: 0.8825\n",
      "Генерация из посева: n crying in \n",
      "n crying in the distance. and she spoke to the last resears, the king said to the project gutenberg-tm electroni==================================================\n",
      "Итерация #: 3\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 70s 441us/step - loss: 0.8396\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 70s 444us/step - loss: 0.7986\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 70s 442us/step - loss: 0.7582\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 69s 433us/step - loss: 0.7211\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 68s 431us/step - loss: 0.68530s\n",
      "Генерация из посева: baby joined)\n",
      "baby joined), and she said the last words of course, the duchess took her edea con anle! the king said to the pr==================================================\n",
      "Итерация #: 4\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 70s 441us/step - loss: 0.6514\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 72s 454us/step - loss: 0.6190\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 69s 435us/step - loss: 0.5889\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 66s 415us/step - loss: 0.5623\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 64s 403us/step - loss: 0.5351\n",
      "Генерация из посева: nd discontin\n",
      "nd discontinue spoke. cried the mock turtle.  an more than nothing else to do, and they all crowded round the co==================================================\n",
      "Итерация #: 5\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 73s 462us/step - loss: 0.5108\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 66s 415us/step - loss: 0.4886\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 64s 406us/step - loss: 0.4668\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 66s 413us/step - loss: 0.44820s - loss\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 67s 423us/step - loss: 0.4304\n",
      "Генерация из посева: hat have no \n",
      "hat have no answers. alice did not find of it, for she felt very curious to see what was that it might not put i==================================================\n",
      "Итерация #: 6\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 66s 416us/step - loss: 0.4128\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 64s 401us/step - loss: 0.3989\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 62s 389us/step - loss: 0.3842\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 62s 391us/step - loss: 0.3735\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 62s 388us/step - loss: 0.3618\n",
      "Генерация из посева: why, said th\n",
      "why, said the gryphon went on, if you dont know where dinn may be, said the gryphon went on, if you dont know wh==================================================\n",
      "Итерация #: 7\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 62s 390us/step - loss: 0.3510\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 62s 389us/step - loss: 0.3423\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 62s 388us/step - loss: 0.3344\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 64s 406us/step - loss: 0.3263\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 62s 389us/step - loss: 0.3191\n",
      "Генерация из посева: n! so they w\n",
      "n! so they went up to the mock turtle as it sudpenly as said nothing. perhaps it doesnt understand english, thou==================================================\n",
      "Итерация #: 8\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 63s 397us/step - loss: 0.3136\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 59s 374us/step - loss: 0.3069\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 59s 373us/step - loss: 0.3035\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 60s 376us/step - loss: 0.2969\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 60s 376us/step - loss: 0.2919\n",
      "Генерация из посева:  the foundat\n",
      " the foundation are tratemark, and any other party distributing a project gutenberg-tm ebooks are often created ==================================================\n",
      "Итерация #: 9\n",
      "Epoch 1/5\n",
      "158771/158771 [==============================] - 59s 374us/step - loss: 0.2877\n",
      "Epoch 2/5\n",
      "158771/158771 [==============================] - 59s 374us/step - loss: 0.2843\n",
      "Epoch 3/5\n",
      "158771/158771 [==============================] - 59s 374us/step - loss: 0.2807\n",
      "Epoch 4/5\n",
      "158771/158771 [==============================] - 59s 374us/step - loss: 0.2767\n",
      "Epoch 5/5\n",
      "158771/158771 [==============================] - 67s 424us/step - loss: 0.2760\n",
      "Генерация из посева: are put out \n",
      "are put out here? she asked. yes, thats it, said the dodo solemnly presented the three gardeners, who was a very\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# построчное чтение из примера с текстом \n",
    "with open(\"alice_in_wonderland.txt\", 'rb') as _in:\n",
    "    lines = []\n",
    "    for line in _in:\n",
    "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        lines.append(line)\n",
    "text = \" \".join(lines)\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "\n",
    "\n",
    "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
    "# ID and a specific character. The numerical ID will correspond to a column\n",
    "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
    "# число при использовании one-hot кодировки для представление входов символов\n",
    "char2index = {c: i for i, c in enumerate(chars)}\n",
    "index2char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# для удобства выберете фиксированную длину последовательность 10 символов \n",
    "SEQLEN, STEP = 12, 1\n",
    "input_chars, label_chars = [], []\n",
    "\n",
    "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i: i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "\n",
    "\n",
    "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
    "\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
    "BATCH_SIZE, HIDDEN_SIZE = 150, 256\n",
    "NUM_ITERATIONS = 10 # 25 должно быть достаточно\n",
    "NUM_EPOCHS_PER_ITERATION = 5\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "\n",
    "\n",
    "# Create a super simple recurrent neural network. There is one recurrent\n",
    "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
    "# encoded input layer. This is followed by a Dense fully-connected layer\n",
    "# across the set of possible next characters, which is converted to a\n",
    "# probability score via a standard softmax activation with a multi-class\n",
    "# cross-entropy loss function linking the prediction to the one-hot\n",
    "# encoding character label.\n",
    "\n",
    "'''\n",
    "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
    "'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, nb_chars),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, char2index[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = index2char[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результат:\n",
    "are put out here? she asked. yes, thats it, said the dodo solemnly presented the three gardeners, who was a very"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Попробуйте на numpy реализовать нейронную сеть архитектуры LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где-то полгода назад я вдоль и поперёк прочёл одну из книг, которые Вы советовали, а именно \"Грокаем Глубокое Обучение\", и последние главы книги посвещены созданию собственного фреймворка глубокого обучения, похожего на pytorch. Вот часть этого фремворка, большую часть я вырезал, оставил то, что нужно для реализации ячейки LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if (id is None):\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "        \n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if (cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if self.autograd:\n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    return\n",
    "                    raise Exception(\"Cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            if self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None):        \n",
    "                if self.creation_op == \"add\":\n",
    "                    self.creators[0].backward(grad, self)\n",
    "                    self.creators[1].backward(grad, self)\n",
    "                if self.creation_op == \"neg\":\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                if self.creation_op == \"sub\":\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                if self.creation_op == \"mul\":\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                if self.creation_op == \"mm\":\n",
    "                    act = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                if self.creation_op == \"transpose\":\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                if \"sum\" in self.creation_op:\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim, ds))\n",
    "                if \"expand\" in self.creation_op:\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                if self.creation_op == \"sigmoid\":\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones-self)))\n",
    "                if self.creation_op == \"tanh\":\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                if self.creation_op == \"index_select\":\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                if self.creation_op == \"cross_entropy\":\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "        \n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "        \n",
    "    def sum(self, dim):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "        \n",
    "    def expand(self, dim, copies):\n",
    "        trans_cmd = list(range(0, len(self.data.shape)))\n",
    "        trans_cmd.insert(dim, len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        if self.autograd:\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self, x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\"\n",
    "                         )\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "    \n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\"\n",
    "                         )\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if self.autograd:\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\"\n",
    "                        )\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp, axis=len(self.data.shape)-1, keepdims=True)\n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        \n",
    "        if self.autograd:\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\"\n",
    "                        )\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        return Tensor(loss)\n",
    "    \n",
    "    def softmax(self):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        return softmax_output\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    def get_parameters(self):\n",
    "        return self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        self.use_bias = bias\n",
    "        W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        if (self.use_bias):\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "        if (self.use_bias):\n",
    "            self.parameters.append(self.bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if (self.use_bias):\n",
    "            return input.mm(self.weight)+self.bias.expand(0, len(input.data))\n",
    "        return input.mm(self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "            \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.alpha * p.grad.data\n",
    "            if zero:\n",
    "                p.grad.data *= 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)\n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()\n",
    "        self.parameters += self.ho.get_parameters()\n",
    "        self.parameters += self.hc.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        prev_hidden = hidden[0]\n",
    "        prev_cell = hidden[1]\n",
    "        \n",
    "        f=(self.xf.forward(input)+self.hf.forward(prev_hidden)).sigmoid()\n",
    "        i=(self.xi.forward(input)+self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o=(self.xo.forward(input)+self.ho.forward(prev_hidden)).sigmoid()\n",
    "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()\n",
    "        c = (f*prev_cell) + (i*g)\n",
    "        h = o*c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        h.data[:, 0] += 1\n",
    "        c.data[:, 0] += 1\n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"alice_in_wonderland.txt\", 'r', encoding='utf-8')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "model.w_ho.weight.data *= 0\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters()+embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int(indices.shape[0] / batch_size)\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int((n_batches-1) / bptt)\n",
    "input_batches = input_batched_indices[:n_bptt*bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)\n",
    "\n",
    "def train(iterations=10):\n",
    "    min_loss = 1000\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        batches_to_train = len(input_batches)\n",
    "        \n",
    "        for batch_i in range(batches_to_train):\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True),\n",
    "                      Tensor(hidden[1].data, autograd=True))\n",
    "\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                if t == 0:\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "            loss = losses[-1]\n",
    "            \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            total_loss += loss.data / bptt\n",
    "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
    "            print(min_loss)\n",
    "            if epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "                print()\n",
    "            log = f\"\\rIter: {iter}\"\n",
    "            log += f\" - Alpha {str(optim.alpha)[0:5]}\"\n",
    "            log += f\" - Batch {batch_i+1}/{len(input_batches)}\"\n",
    "            log += f\" - Min Loss: {str(min_loss)[0:5]}\"\n",
    "            log += f\" - Loss: {epoch_loss}\"\n",
    "            if batch_i == 0:\n",
    "                log += \" - \" + generate_sample(70, 'T').replace('\\n', ' ')\n",
    "            sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()\n",
    "\n",
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor([word2index[init_char]])\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor([m])\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 1/409 - Min Loss: 85.99 - Loss: 85.99999999999982 - oo                        hfffDBjoooooooo                             85.99999999999982\n",
      "Iter: 0 - Alpha 0.05 - Batch 2/409 - Min Loss: 85.99 - Loss: 86.0001919677186685.99999999999982\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 3/409 - Min Loss: 85.98 - Loss: 85.9822391118870185.98223911188701\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 4/409 - Min Loss: 85.95 - Loss: 85.9548203156825185.95482031568251\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 5/409 - Min Loss: 85.90 - Loss: 85.902958807145685.9029588071456\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 6/409 - Min Loss: 85.78 - Loss: 85.7856351769521785.78563517695217\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 7/409 - Min Loss: 85.51 - Loss: 85.5140117644676785.51401176446767\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 8/409 - Min Loss: 84.96 - Loss: 84.9655543900428784.96555439004287\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 9/409 - Min Loss: 83.57 - Loss: 83.5791042178419683.57910421784196\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 10/409 - Min Loss: 80.61 - Loss: 80.6178945257874980.61789452578749\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 11/409 - Min Loss: 75.68 - Loss: 75.6880896215571375.68808962155713\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 12/409 - Min Loss: 71.05 - Loss: 71.0530366513850471.05303665138504\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 13/409 - Min Loss: 67.63 - Loss: 67.6326183373594267.63261833735942\n",
      "Iter: 0 - Alpha 0.05 - Batch 14/409 - Min Loss: 67.63 - Loss: 67.9175059233587367.63261833735942\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 15/409 - Min Loss: 66.61 - Loss: 66.6186843783648966.61868437836489\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 16/409 - Min Loss: 64.15 - Loss: 64.1507480871312564.15074808713125\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 17/409 - Min Loss: 61.24 - Loss: 61.2477160603195561.24771606031955\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 18/409 - Min Loss: 58.56 - Loss: 58.5611303468889758.56113034688897\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 19/409 - Min Loss: 57.41 - Loss: 57.4139076548542257.41390765485422\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 20/409 - Min Loss: 55.73 - Loss: 55.73360301237667655.733603012376676\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 21/409 - Min Loss: 53.76 - Loss: 53.7610976622451453.76109766224514\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 22/409 - Min Loss: 52.15 - Loss: 52.1546199271416252.15461992714162\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 23/409 - Min Loss: 50.76 - Loss: 50.7665155464915250.76651554649152\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 24/409 - Min Loss: 49.70 - Loss: 49.7008839561557549.70088395615575\n",
      "Iter: 0 - Alpha 0.05 - Batch 25/409 - Min Loss: 49.70 - Loss: 49.7920224129626949.70088395615575\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 26/409 - Min Loss: 49.34 - Loss: 49.3453649735231749.34536497352317\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 27/409 - Min Loss: 48.32 - Loss: 48.3205714376077348.32057143760773\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 28/409 - Min Loss: 47.40 - Loss: 47.4030911372000947.40309113720009\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 29/409 - Min Loss: 46.73 - Loss: 46.7347568767598946.73475687675989\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 30/409 - Min Loss: 45.89 - Loss: 45.89421443426629645.894214434266296\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 31/409 - Min Loss: 45.55 - Loss: 45.5501217146174445.55012171461744\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 32/409 - Min Loss: 44.75 - Loss: 44.7576703054894144.75767030548941\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 33/409 - Min Loss: 44.04 - Loss: 44.0412120621238644.04121206212386\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 34/409 - Min Loss: 43.35 - Loss: 43.3506713199902943.35067131999029\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 35/409 - Min Loss: 42.65 - Loss: 42.6537824765843642.65378247658436\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 36/409 - Min Loss: 41.99 - Loss: 41.9976413259818441.99764132598184\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 37/409 - Min Loss: 41.58 - Loss: 41.5807280287806741.58072802878067\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 38/409 - Min Loss: 41.36 - Loss: 41.3616762849620341.36167628496203\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 39/409 - Min Loss: 40.79 - Loss: 40.79104317197748540.791043171977485\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 40/409 - Min Loss: 40.30 - Loss: 40.3063682937112340.30636829371123\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 41/409 - Min Loss: 39.77 - Loss: 39.7774887562658239.77748875626582\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 42/409 - Min Loss: 39.26 - Loss: 39.26313068260872539.263130682608725\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 43/409 - Min Loss: 38.73 - Loss: 38.7365741353277738.73657413532777\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 44/409 - Min Loss: 38.24 - Loss: 38.2428449027299738.24284490272997\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 45/409 - Min Loss: 37.80 - Loss: 37.8078377431008537.80783774310085\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 46/409 - Min Loss: 37.52 - Loss: 37.5289480166912237.52894801669122\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 47/409 - Min Loss: 37.04 - Loss: 37.0426269269764437.04262692697644\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 48/409 - Min Loss: 36.62 - Loss: 36.62681263815585536.626812638155855\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 49/409 - Min Loss: 36.27 - Loss: 36.2771572838588436.27715728385884\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 50/409 - Min Loss: 35.87 - Loss: 35.8722572875496435.87225728754964\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 51/409 - Min Loss: 35.61 - Loss: 35.61820534002857435.618205340028574\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 52/409 - Min Loss: 35.44 - Loss: 35.4433776574240635.44337765742406\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 53/409 - Min Loss: 35.40 - Loss: 35.40318430410725635.403184304107256\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 54/409 - Min Loss: 35.18 - Loss: 35.1880608271571935.18806082715719\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 55/409 - Min Loss: 34.93 - Loss: 34.9308595844280734.93085958442807\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 56/409 - Min Loss: 34.66 - Loss: 34.6652202281078634.66522022810786\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 57/409 - Min Loss: 34.32 - Loss: 34.32730806405566434.327308064055664\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 58/409 - Min Loss: 34.03 - Loss: 34.03283988772409434.032839887724094\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 59/409 - Min Loss: 33.77 - Loss: 33.77537111548555533.775371115485555\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 60/409 - Min Loss: 33.56 - Loss: 33.5627171602020433.56271716020204\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 61/409 - Min Loss: 33.25 - Loss: 33.2594196626791633.25941966267916\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 62/409 - Min Loss: 33.01 - Loss: 33.0133415997388933.01334159973889\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 63/409 - Min Loss: 32.68 - Loss: 32.6878460517458332.68784605174583\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 64/409 - Min Loss: 32.38 - Loss: 32.38461525119405632.384615251194056\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 65/409 - Min Loss: 32.08 - Loss: 32.083885148515532.0838851485155\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 66/409 - Min Loss: 31.81 - Loss: 31.81895561582846531.818955615828465\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 67/409 - Min Loss: 31.79 - Loss: 31.7984949708360931.79849497083609\n",
      "Iter: 0 - Alpha 0.05 - Batch 68/409 - Min Loss: 31.79 - Loss: 32.12174329029665631.79849497083609\n",
      "Iter: 0 - Alpha 0.05 - Batch 69/409 - Min Loss: 31.79 - Loss: 32.2453250844577331.79849497083609\n",
      "Iter: 0 - Alpha 0.05 - Batch 70/409 - Min Loss: 31.79 - Loss: 32.0786652222908931.79849497083609\n",
      "Iter: 0 - Alpha 0.05 - Batch 71/409 - Min Loss: 31.79 - Loss: 31.88490707389173231.79849497083609\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 72/409 - Min Loss: 31.66 - Loss: 31.66733092034074231.667330920340742\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 73/409 - Min Loss: 31.51 - Loss: 31.51805832982504531.518058329825045\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 74/409 - Min Loss: 31.42 - Loss: 31.42984460492308331.429844604923083\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 75/409 - Min Loss: 31.34 - Loss: 31.34120543027616531.341205430276165\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 76/409 - Min Loss: 31.22 - Loss: 31.228826171788731.2288261717887\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 77/409 - Min Loss: 31.06 - Loss: 31.06115279131085731.061152791310857\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 78/409 - Min Loss: 31.00 - Loss: 31.0015949456503231.00159494565032\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 79/409 - Min Loss: 30.80 - Loss: 30.808528380671930.8085283806719\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 80/409 - Min Loss: 30.64 - Loss: 30.64654811737986530.646548117379865\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 81/409 - Min Loss: 30.58 - Loss: 30.58972182793871430.589721827938714\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 82/409 - Min Loss: 30.46 - Loss: 30.46001001946973530.460010019469735\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha 0.05 - Batch 83/409 - Min Loss: 30.28 - Loss: 30.2825990812049730.28259908120497\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 84/409 - Min Loss: 30.08 - Loss: 30.08294720407613530.082947204076135\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 85/409 - Min Loss: 29.88 - Loss: 29.88100790213148629.881007902131486\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 86/409 - Min Loss: 29.71 - Loss: 29.711191557570729.7111915575707\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 87/409 - Min Loss: 29.56 - Loss: 29.56141420563840729.561414205638407\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 88/409 - Min Loss: 29.43 - Loss: 29.4343184628692129.43431846286921\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 89/409 - Min Loss: 29.20 - Loss: 29.20866480526646629.208664805266466\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 90/409 - Min Loss: 29.01 - Loss: 29.01765813570825329.017658135708253\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 91/409 - Min Loss: 28.91 - Loss: 28.91257799842601628.912577998426016\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 92/409 - Min Loss: 28.77 - Loss: 28.77782025269426628.777820252694266\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 93/409 - Min Loss: 28.62 - Loss: 28.62218953580809428.622189535808094\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 94/409 - Min Loss: 28.47 - Loss: 28.47644933153298728.476449331532987\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 95/409 - Min Loss: 28.36 - Loss: 28.36935798516928728.369357985169287\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 96/409 - Min Loss: 28.30 - Loss: 28.3022302720969728.30223027209697\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 97/409 - Min Loss: 28.21 - Loss: 28.21482774205959428.214827742059594\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 98/409 - Min Loss: 28.13 - Loss: 28.13150710411952528.131507104119525\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 99/409 - Min Loss: 28.00 - Loss: 28.00018985377865428.000189853778654\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 100/409 - Min Loss: 27.85 - Loss: 27.8515004164330527.85150041643305\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 101/409 - Min Loss: 27.77 - Loss: 27.7787650718401927.77876507184019\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 102/409 - Min Loss: 27.69 - Loss: 27.6906110269536227.69061102695362\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 103/409 - Min Loss: 27.57 - Loss: 27.5764606086826727.57646060868267\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 104/409 - Min Loss: 27.38 - Loss: 27.38602155896700627.386021558967006\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 105/409 - Min Loss: 27.27 - Loss: 27.2733136870869927.27331368708699\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 106/409 - Min Loss: 27.13 - Loss: 27.13373598223281427.133735982232814\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 107/409 - Min Loss: 26.98 - Loss: 26.98265003972929326.982650039729293\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 108/409 - Min Loss: 26.83 - Loss: 26.83105261090124626.831052610901246\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 109/409 - Min Loss: 26.69 - Loss: 26.69517452587127726.695174525871277\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 110/409 - Min Loss: 26.61 - Loss: 26.61242992389137626.612429923891376\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 111/409 - Min Loss: 26.54 - Loss: 26.5419818187855526.54198181878555\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 112/409 - Min Loss: 26.46 - Loss: 26.46467755943312726.464677559433127\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 113/409 - Min Loss: 26.34 - Loss: 26.3421250941508526.34212509415085\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 114/409 - Min Loss: 26.30 - Loss: 26.30457597843338426.304575978433384\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 115/409 - Min Loss: 26.24 - Loss: 26.24723291618712726.247232916187127\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 116/409 - Min Loss: 26.20 - Loss: 26.2029698484222826.20296984842228\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 117/409 - Min Loss: 26.17 - Loss: 26.17605186985882826.176051869858828\n",
      "Iter: 0 - Alpha 0.05 - Batch 118/409 - Min Loss: 26.17 - Loss: 26.1854021119663826.176051869858828\n",
      "Iter: 0 - Alpha 0.05 - Batch 119/409 - Min Loss: 26.17 - Loss: 26.1820180285930326.176051869858828\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 120/409 - Min Loss: 26.14 - Loss: 26.1429813412078626.14298134120786\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 121/409 - Min Loss: 26.09 - Loss: 26.09671788649874526.096717886498745\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 122/409 - Min Loss: 26.06 - Loss: 26.0669998078193626.06699980781936\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 123/409 - Min Loss: 25.97 - Loss: 25.9768049397850325.97680493978503\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 124/409 - Min Loss: 25.89 - Loss: 25.8964229819309125.89642298193091\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 125/409 - Min Loss: 25.81 - Loss: 25.81026885325869525.810268853258695\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 126/409 - Min Loss: 25.74 - Loss: 25.74709324128818725.747093241288187\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 127/409 - Min Loss: 25.67 - Loss: 25.6747007347687625.67470073476876\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 128/409 - Min Loss: 25.62 - Loss: 25.62907534982973325.629075349829733\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 129/409 - Min Loss: 25.56 - Loss: 25.56826782085127225.568267820851272\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 130/409 - Min Loss: 25.48 - Loss: 25.48369167465437725.483691674654377\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 131/409 - Min Loss: 25.42 - Loss: 25.4258720277996925.42587202779969\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 132/409 - Min Loss: 25.34 - Loss: 25.3425545393764525.34255453937645\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 133/409 - Min Loss: 25.28 - Loss: 25.28272632265247225.282726322652472\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 134/409 - Min Loss: 25.19 - Loss: 25.1914713924283725.19147139242837\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 135/409 - Min Loss: 25.14 - Loss: 25.1456122408768825.14561224087688\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 136/409 - Min Loss: 25.03 - Loss: 25.0318048492656825.03180484926568\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 137/409 - Min Loss: 24.94 - Loss: 24.94679322821967624.946793228219676\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 138/409 - Min Loss: 24.83 - Loss: 24.8305467892531824.83054678925318\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 139/409 - Min Loss: 24.74 - Loss: 24.74850711274393324.748507112743933\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 140/409 - Min Loss: 24.69 - Loss: 24.6936533106249324.69365331062493\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 141/409 - Min Loss: 24.68 - Loss: 24.68862743415059724.688627434150597\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 142/409 - Min Loss: 24.60 - Loss: 24.60933875607682424.609338756076824\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 143/409 - Min Loss: 24.54 - Loss: 24.54172807191016824.541728071910168\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 144/409 - Min Loss: 24.45 - Loss: 24.457751587866324.4577515878663\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 145/409 - Min Loss: 24.40 - Loss: 24.4094520080212224.40945200802122\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 146/409 - Min Loss: 24.32 - Loss: 24.3280249470804124.32802494708041\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 147/409 - Min Loss: 24.27 - Loss: 24.27787478021343824.277874780213438\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 148/409 - Min Loss: 24.20 - Loss: 24.2069947854872424.20699478548724\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 149/409 - Min Loss: 24.14 - Loss: 24.14268133118150324.142681331181503\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 150/409 - Min Loss: 24.10 - Loss: 24.105731720457924.1057317204579\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 151/409 - Min Loss: 24.01 - Loss: 24.01631798439119324.016317984391193\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 152/409 - Min Loss: 23.96 - Loss: 23.96714276550265823.967142765502658\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 153/409 - Min Loss: 23.89 - Loss: 23.89808164772617323.898081647726173\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 154/409 - Min Loss: 23.84 - Loss: 23.84630182237480523.846301822374805\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 155/409 - Min Loss: 23.78 - Loss: 23.78822935173324323.788229351733243\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 156/409 - Min Loss: 23.73 - Loss: 23.739686302996223.7396863029962\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 157/409 - Min Loss: 23.67 - Loss: 23.6785357676867823.67853576768678\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 158/409 - Min Loss: 23.61 - Loss: 23.61800664288628223.618006642886282\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 159/409 - Min Loss: 23.57 - Loss: 23.5704905155629223.57049051556292\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 160/409 - Min Loss: 23.49 - Loss: 23.4970082705611723.49700827056117\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 161/409 - Min Loss: 23.41 - Loss: 23.4195864131161923.41958641311619\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 162/409 - Min Loss: 23.34 - Loss: 23.349814480394823.3498144803948\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 163/409 - Min Loss: 23.26 - Loss: 23.26610467675048423.266104676750484\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 164/409 - Min Loss: 23.20 - Loss: 23.20585800645232423.205858006452324\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha 0.05 - Batch 165/409 - Min Loss: 23.14 - Loss: 23.14827937952868523.148279379528685\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 166/409 - Min Loss: 23.09 - Loss: 23.09287822788470723.092878227884707\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 167/409 - Min Loss: 23.06 - Loss: 23.06549520187888523.065495201878885\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 168/409 - Min Loss: 23.02 - Loss: 23.0251248584070623.02512485840706\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 169/409 - Min Loss: 22.97 - Loss: 22.97837436460518422.978374364605184\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 170/409 - Min Loss: 22.92 - Loss: 22.92648297423329522.926482974233295\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 171/409 - Min Loss: 22.86 - Loss: 22.86520718438011822.865207184380118\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 172/409 - Min Loss: 22.81 - Loss: 22.81389646607698722.813896466076987\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 173/409 - Min Loss: 22.76 - Loss: 22.76271739784147722.762717397841477\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 174/409 - Min Loss: 22.69 - Loss: 22.6959328443422122.69593284434221\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 175/409 - Min Loss: 22.63 - Loss: 22.6341151494774122.63411514947741\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 176/409 - Min Loss: 22.58 - Loss: 22.5849284976678122.58492849766781\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 177/409 - Min Loss: 22.52 - Loss: 22.52398697371100422.523986973711004\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 178/409 - Min Loss: 22.48 - Loss: 22.4894957455018622.48949574550186\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 179/409 - Min Loss: 22.43 - Loss: 22.43316589768840822.433165897688408\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 180/409 - Min Loss: 22.37 - Loss: 22.37118078276142422.371180782761424\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 181/409 - Min Loss: 22.31 - Loss: 22.31733014202846422.317330142028464\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 182/409 - Min Loss: 22.26 - Loss: 22.26643383624175622.266433836241756\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 183/409 - Min Loss: 22.21 - Loss: 22.2186965009218522.21869650092185\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 184/409 - Min Loss: 22.15 - Loss: 22.15138715542879722.151387155428797\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 185/409 - Min Loss: 22.10 - Loss: 22.10140775200037322.101407752000373\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 186/409 - Min Loss: 22.06 - Loss: 22.06854137651385622.068541376513856\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 187/409 - Min Loss: 22.02 - Loss: 22.02660625269580722.026606252695807\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 188/409 - Min Loss: 21.97 - Loss: 21.97560936725706821.975609367257068\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 189/409 - Min Loss: 21.90 - Loss: 21.90850040390894521.908500403908945\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 190/409 - Min Loss: 21.88 - Loss: 21.88499909260924521.884999092609245\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 191/409 - Min Loss: 21.86 - Loss: 21.86323681553931721.863236815539317\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 192/409 - Min Loss: 21.83 - Loss: 21.83382229310468521.833822293104685\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 193/409 - Min Loss: 21.77 - Loss: 21.77811786866606521.778117868666065\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 194/409 - Min Loss: 21.75 - Loss: 21.75563629217037221.755636292170372\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 195/409 - Min Loss: 21.71 - Loss: 21.71928199018944721.719281990189447\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 196/409 - Min Loss: 21.70 - Loss: 21.701139194126121.7011391941261\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 197/409 - Min Loss: 21.65 - Loss: 21.6574202723972621.65742027239726\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 198/409 - Min Loss: 21.62 - Loss: 21.62754812483563821.627548124835638\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 199/409 - Min Loss: 21.58 - Loss: 21.58835161997756221.588351619977562\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 200/409 - Min Loss: 21.53 - Loss: 21.53711426755283821.537114267552838\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 201/409 - Min Loss: 21.50 - Loss: 21.50230326391769621.502303263917696\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 202/409 - Min Loss: 21.47 - Loss: 21.47248724029230321.472487240292303\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 203/409 - Min Loss: 21.45 - Loss: 21.45416302104680621.454163021046806\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 204/409 - Min Loss: 21.41 - Loss: 21.41360098287259821.413600982872598\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 205/409 - Min Loss: 21.38 - Loss: 21.38018981230148421.380189812301484\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 206/409 - Min Loss: 21.33 - Loss: 21.33504176980220621.335041769802206\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 207/409 - Min Loss: 21.28 - Loss: 21.28566440021296221.285664400212962\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 208/409 - Min Loss: 21.23 - Loss: 21.2381020440765721.23810204407657\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 209/409 - Min Loss: 21.18 - Loss: 21.1816814445588721.18168144455887\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 210/409 - Min Loss: 21.14 - Loss: 21.1497008563184421.14970085631844\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 211/409 - Min Loss: 21.14 - Loss: 21.14500851994750621.145008519947506\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 212/409 - Min Loss: 21.11 - Loss: 21.1198000408874521.11980004088745\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 213/409 - Min Loss: 21.07 - Loss: 21.0787786595590821.07877865955908\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 214/409 - Min Loss: 21.05 - Loss: 21.05509380705130321.055093807051303\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 215/409 - Min Loss: 21.02 - Loss: 21.0223326905365221.02233269053652\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 216/409 - Min Loss: 20.98 - Loss: 20.98256002700519220.982560027005192\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 217/409 - Min Loss: 20.97 - Loss: 20.97329485267535520.973294852675355\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 218/409 - Min Loss: 20.94 - Loss: 20.9418330874601520.94183308746015\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 219/409 - Min Loss: 20.91 - Loss: 20.91725292944718720.917252929447187\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 220/409 - Min Loss: 20.89 - Loss: 20.8919114476006720.89191144760067\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 221/409 - Min Loss: 20.87 - Loss: 20.8700947542450320.87009475424503\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 222/409 - Min Loss: 20.82 - Loss: 20.8254449336579820.82544493365798\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 223/409 - Min Loss: 20.79 - Loss: 20.7924654440658720.79246544406587\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 224/409 - Min Loss: 20.75 - Loss: 20.75906523376106320.759065233761063\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 225/409 - Min Loss: 20.72 - Loss: 20.72353956344579720.723539563445797\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 226/409 - Min Loss: 20.68 - Loss: 20.6866829348393220.68668293483932\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 227/409 - Min Loss: 20.66 - Loss: 20.66464968905067320.664649689050673\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 228/409 - Min Loss: 20.62 - Loss: 20.62761059709736220.627610597097362\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 229/409 - Min Loss: 20.60 - Loss: 20.60413371714949420.604133717149494\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 230/409 - Min Loss: 20.57 - Loss: 20.5730041987178620.57300419871786\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 231/409 - Min Loss: 20.54 - Loss: 20.54684108254668520.546841082546685\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 232/409 - Min Loss: 20.52 - Loss: 20.52025262985729320.520252629857293\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 233/409 - Min Loss: 20.49 - Loss: 20.49667080613857820.496670806138578\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 234/409 - Min Loss: 20.46 - Loss: 20.4653959316777820.46539593167778\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 235/409 - Min Loss: 20.45 - Loss: 20.45684772250470520.456847722504705\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 236/409 - Min Loss: 20.42 - Loss: 20.4279502045394620.42795020453946\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 237/409 - Min Loss: 20.38 - Loss: 20.38584411140730520.385844111407305\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 238/409 - Min Loss: 20.34 - Loss: 20.34188893758359720.341888937583597\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 239/409 - Min Loss: 20.29 - Loss: 20.29714452862190720.297144528621907\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 240/409 - Min Loss: 20.27 - Loss: 20.27774105405435320.277741054054353\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 241/409 - Min Loss: 20.24 - Loss: 20.24839898326770220.248398983267702\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 242/409 - Min Loss: 20.20 - Loss: 20.20801337395158720.208013373951587\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 243/409 - Min Loss: 20.18 - Loss: 20.1833820376074320.18338203760743\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 244/409 - Min Loss: 20.15 - Loss: 20.15609428105520420.156094281055204\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 245/409 - Min Loss: 20.12 - Loss: 20.12864053658683320.128640536586833\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha 0.05 - Batch 246/409 - Min Loss: 20.09 - Loss: 20.09991814927855520.099918149278555\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 247/409 - Min Loss: 20.07 - Loss: 20.07078886250372620.070788862503726\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 248/409 - Min Loss: 20.06 - Loss: 20.069167826707820.0691678267078\n",
      "Iter: 0 - Alpha 0.05 - Batch 249/409 - Min Loss: 20.06 - Loss: 20.0890462876021320.0691678267078\n",
      "Iter: 0 - Alpha 0.05 - Batch 250/409 - Min Loss: 20.06 - Loss: 20.08113309803974620.0691678267078\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 251/409 - Min Loss: 20.04 - Loss: 20.04949275438167820.049492754381678\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 252/409 - Min Loss: 20.01 - Loss: 20.01829572603667620.018295726036676\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 253/409 - Min Loss: 19.98 - Loss: 19.98665670769971619.986656707699716\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 254/409 - Min Loss: 19.95 - Loss: 19.95438804658179419.954388046581794\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 255/409 - Min Loss: 19.92 - Loss: 19.92822169945914719.928221699459147\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 256/409 - Min Loss: 19.90 - Loss: 19.9050983201961219.90509832019612\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 257/409 - Min Loss: 19.86 - Loss: 19.8680212070780519.86802120707805\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 258/409 - Min Loss: 19.83 - Loss: 19.8325205817155119.83252058171551\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 259/409 - Min Loss: 19.79 - Loss: 19.79485004355843419.794850043558434\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 260/409 - Min Loss: 19.76 - Loss: 19.7609230437612219.76092304376122\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 261/409 - Min Loss: 19.73 - Loss: 19.731456952911319.7314569529113\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 262/409 - Min Loss: 19.72 - Loss: 19.7214028804246519.72140288042465\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 263/409 - Min Loss: 19.69 - Loss: 19.6953238884585719.69532388845857\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 264/409 - Min Loss: 19.67 - Loss: 19.67639771970308619.676397719703086\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 265/409 - Min Loss: 19.65 - Loss: 19.65352332328215519.653523323282155\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 266/409 - Min Loss: 19.65 - Loss: 19.65261424388658219.652614243886582\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 267/409 - Min Loss: 19.63 - Loss: 19.63141986554592619.631419865545926\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 268/409 - Min Loss: 19.62 - Loss: 19.62780261034102519.627802610341025\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 269/409 - Min Loss: 19.61 - Loss: 19.61447896293006519.614478962930065\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 270/409 - Min Loss: 19.59 - Loss: 19.59309553020048619.593095530200486\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 271/409 - Min Loss: 19.59 - Loss: 19.59110992949982619.591109929499826\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 272/409 - Min Loss: 19.57 - Loss: 19.5725485873361619.57254858733616\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 273/409 - Min Loss: 19.56 - Loss: 19.56056705353612619.560567053536126\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 274/409 - Min Loss: 19.54 - Loss: 19.54417074269119319.544170742691193\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 275/409 - Min Loss: 19.51 - Loss: 19.51758797782877519.517587977828775\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 276/409 - Min Loss: 19.48 - Loss: 19.4845377654734819.48453776547348\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 277/409 - Min Loss: 19.45 - Loss: 19.45730009674407819.457300096744078\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 278/409 - Min Loss: 19.44 - Loss: 19.4462574765624219.44625747656242\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 279/409 - Min Loss: 19.41 - Loss: 19.41670338471056219.416703384710562\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 280/409 - Min Loss: 19.39 - Loss: 19.39535687929611619.395356879296116\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 281/409 - Min Loss: 19.38 - Loss: 19.38019023401703219.380190234017032\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 282/409 - Min Loss: 19.35 - Loss: 19.35547088876714719.355470888767147\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 283/409 - Min Loss: 19.33 - Loss: 19.33640236523333319.336402365233333\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 284/409 - Min Loss: 19.32 - Loss: 19.32143464817033519.321434648170335\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 285/409 - Min Loss: 19.30 - Loss: 19.3052824950136819.30528249501368\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 286/409 - Min Loss: 19.29 - Loss: 19.29065483385574719.290654833855747\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 287/409 - Min Loss: 19.27 - Loss: 19.27180053479421719.271800534794217\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 288/409 - Min Loss: 19.26 - Loss: 19.26895204588604219.268952045886042\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 289/409 - Min Loss: 19.25 - Loss: 19.25402672935452519.254026729354525\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 290/409 - Min Loss: 19.23 - Loss: 19.23634465935159819.236344659351598\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 291/409 - Min Loss: 19.22 - Loss: 19.22191180755430719.221911807554307\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 292/409 - Min Loss: 19.20 - Loss: 19.2074780957032619.20747809570326\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 293/409 - Min Loss: 19.18 - Loss: 19.18481368063953619.184813680639536\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 294/409 - Min Loss: 19.15 - Loss: 19.156313682628419.1563136826284\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 295/409 - Min Loss: 19.13 - Loss: 19.1310545932608719.13105459326087\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 296/409 - Min Loss: 19.10 - Loss: 19.1055111980496419.10551119804964\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 297/409 - Min Loss: 19.08 - Loss: 19.08977712268842519.089777122688425\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 298/409 - Min Loss: 19.07 - Loss: 19.0726280049504419.07262800495044\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 299/409 - Min Loss: 19.06 - Loss: 19.06499597928976219.064995979289762\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 300/409 - Min Loss: 19.05 - Loss: 19.05294193633586319.052941936335863\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 301/409 - Min Loss: 19.03 - Loss: 19.0380068341395419.03800683413954\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 302/409 - Min Loss: 19.01 - Loss: 19.01327759145756219.013277591457562\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 303/409 - Min Loss: 18.98 - Loss: 18.9873827790851218.98738277908512\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 304/409 - Min Loss: 18.97 - Loss: 18.97093030809465518.970930308094655\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 305/409 - Min Loss: 18.95 - Loss: 18.9595678453620818.95956784536208\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 306/409 - Min Loss: 18.94 - Loss: 18.94152710130467618.941527101304676\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 307/409 - Min Loss: 18.91 - Loss: 18.91861669904572818.918616699045728\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 308/409 - Min Loss: 18.89 - Loss: 18.89490880044279218.894908800442792\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 309/409 - Min Loss: 18.87 - Loss: 18.87367786712765418.873677867127654\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 310/409 - Min Loss: 18.84 - Loss: 18.8419901801766718.84199018017667\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 311/409 - Min Loss: 18.81 - Loss: 18.81661564656960618.816615646569606\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 312/409 - Min Loss: 18.78 - Loss: 18.78866554873149818.788665548731498\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 313/409 - Min Loss: 18.76 - Loss: 18.7630795073623218.76307950736232\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 314/409 - Min Loss: 18.75 - Loss: 18.75174503254928718.751745032549287\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 315/409 - Min Loss: 18.73 - Loss: 18.73246741820446518.732467418204465\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 316/409 - Min Loss: 18.69 - Loss: 18.6996906467396718.69969064673967\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 317/409 - Min Loss: 18.68 - Loss: 18.68079196994281718.680791969942817\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 318/409 - Min Loss: 18.66 - Loss: 18.6604574976047218.66045749760472\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 319/409 - Min Loss: 18.64 - Loss: 18.64841785533039318.648417855330393\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 320/409 - Min Loss: 18.63 - Loss: 18.63522475458476218.635224754584762\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 321/409 - Min Loss: 18.61 - Loss: 18.61580542163405718.615805421634057\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 322/409 - Min Loss: 18.59 - Loss: 18.59788862007335218.597888620073352\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 323/409 - Min Loss: 18.57 - Loss: 18.57402256853499318.574022568534993\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 324/409 - Min Loss: 18.55 - Loss: 18.55496725668671618.554967256686716\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 325/409 - Min Loss: 18.54 - Loss: 18.54172817355898718.541728173558987\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 326/409 - Min Loss: 18.52 - Loss: 18.52128346036418818.521283460364188\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 - Alpha 0.05 - Batch 327/409 - Min Loss: 18.51 - Loss: 18.51745991541423618.517459915414236\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 328/409 - Min Loss: 18.51 - Loss: 18.51090785437696318.510907854376963\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 329/409 - Min Loss: 18.50 - Loss: 18.50126867856504718.501268678565047\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 330/409 - Min Loss: 18.47 - Loss: 18.4747336182115618.47473361821156\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 331/409 - Min Loss: 18.45 - Loss: 18.45142861635042718.451428616350427\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 332/409 - Min Loss: 18.44 - Loss: 18.4447675386504818.44476753865048\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 333/409 - Min Loss: 18.42 - Loss: 18.4255973981064418.42559739810644\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 334/409 - Min Loss: 18.40 - Loss: 18.4060525007668118.40605250076681\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 335/409 - Min Loss: 18.38 - Loss: 18.389140791295518.3891407912955\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 336/409 - Min Loss: 18.36 - Loss: 18.3647658740023618.36476587400236\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 337/409 - Min Loss: 18.35 - Loss: 18.35258122939877618.352581229398776\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 338/409 - Min Loss: 18.33 - Loss: 18.33853515709099518.338535157090995\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 339/409 - Min Loss: 18.32 - Loss: 18.32543706713574518.325437067135745\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 340/409 - Min Loss: 18.30 - Loss: 18.3063044955724218.30630449557242\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 341/409 - Min Loss: 18.28 - Loss: 18.2825980342188218.28259803421882\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 342/409 - Min Loss: 18.26 - Loss: 18.2671513089970118.26715130899701\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 343/409 - Min Loss: 18.24 - Loss: 18.2489670256617618.24896702566176\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 344/409 - Min Loss: 18.22 - Loss: 18.2288496870702318.22884968707023\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 345/409 - Min Loss: 18.20 - Loss: 18.20680173729457618.206801737294576\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 346/409 - Min Loss: 18.18 - Loss: 18.18918275361936318.189182753619363\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 347/409 - Min Loss: 18.16 - Loss: 18.1682493433734318.16824934337343\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 348/409 - Min Loss: 18.14 - Loss: 18.14989238018492718.149892380184927\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 349/409 - Min Loss: 18.12 - Loss: 18.12605826845487318.126058268454873\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 350/409 - Min Loss: 18.10 - Loss: 18.10723510154628418.107235101546284\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 351/409 - Min Loss: 18.09 - Loss: 18.09379661491544418.093796614915444\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 352/409 - Min Loss: 18.08 - Loss: 18.0805328396872418.08053283968724\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 353/409 - Min Loss: 18.06 - Loss: 18.0672322224400518.06723222244005\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 354/409 - Min Loss: 18.05 - Loss: 18.0576458256792418.05764582567924\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 355/409 - Min Loss: 18.03 - Loss: 18.03865427967997818.038654279679978\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 356/409 - Min Loss: 18.02 - Loss: 18.0204261177643618.02042611776436\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 357/409 - Min Loss: 18.00 - Loss: 18.00460294866646318.004602948666463\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 358/409 - Min Loss: 17.99 - Loss: 17.99737245171906617.997372451719066\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 359/409 - Min Loss: 17.97 - Loss: 17.9796883856229317.97968838562293\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 360/409 - Min Loss: 17.97 - Loss: 17.97084372071491717.970843720714917\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 361/409 - Min Loss: 17.96 - Loss: 17.96380214244707617.963802142447076\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 362/409 - Min Loss: 17.94 - Loss: 17.9427495232174117.94274952321741\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 363/409 - Min Loss: 17.93 - Loss: 17.93039599163626417.930395991636264\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 364/409 - Min Loss: 17.92 - Loss: 17.92112688397788717.921126883977887\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 365/409 - Min Loss: 17.90 - Loss: 17.90926769267674217.909267692676742\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 366/409 - Min Loss: 17.89 - Loss: 17.89609553270609517.896095532706095\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 367/409 - Min Loss: 17.87 - Loss: 17.87595158733099517.875951587330995\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 368/409 - Min Loss: 17.86 - Loss: 17.8686192075760517.86861920757605\n",
      "Iter: 0 - Alpha 0.05 - Batch 369/409 - Min Loss: 17.86 - Loss: 17.87931411457607217.86861920757605\n",
      "Iter: 0 - Alpha 0.05 - Batch 370/409 - Min Loss: 17.86 - Loss: 17.8722072496195117.86861920757605\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 371/409 - Min Loss: 17.86 - Loss: 17.86522292354524717.865222923545247\n",
      "Iter: 0 - Alpha 0.05 - Batch 372/409 - Min Loss: 17.86 - Loss: 17.87491780595881317.865222923545247\n",
      "Iter: 0 - Alpha 0.05 - Batch 373/409 - Min Loss: 17.86 - Loss: 17.87642485950939517.865222923545247\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 374/409 - Min Loss: 17.86 - Loss: 17.8625025273161717.86250252731617\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 375/409 - Min Loss: 17.84 - Loss: 17.8458956976730417.84589569767304\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 376/409 - Min Loss: 17.84 - Loss: 17.84367176417546517.843671764175465\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 377/409 - Min Loss: 17.84 - Loss: 17.84185983028440217.841859830284402\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 378/409 - Min Loss: 17.83 - Loss: 17.83498707982636217.834987079826362\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 379/409 - Min Loss: 17.83 - Loss: 17.83353984635728717.833539846357287\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 380/409 - Min Loss: 17.82 - Loss: 17.8221543285796617.82215432857966\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 381/409 - Min Loss: 17.81 - Loss: 17.81463336047273717.814633360472737\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 382/409 - Min Loss: 17.80 - Loss: 17.80946894424607417.809468944246074\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 383/409 - Min Loss: 17.80 - Loss: 17.805639207406117.8056392074061\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 384/409 - Min Loss: 17.79 - Loss: 17.79560396925937517.795603969259375\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 385/409 - Min Loss: 17.78 - Loss: 17.78758575929819617.787585759298196\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 386/409 - Min Loss: 17.77 - Loss: 17.77703394874629717.777033948746297\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 387/409 - Min Loss: 17.76 - Loss: 17.76886863148247617.768868631482476\n",
      "Iter: 0 - Alpha 0.05 - Batch 388/409 - Min Loss: 17.76 - Loss: 17.77320828664413517.768868631482476\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 389/409 - Min Loss: 17.76 - Loss: 17.76180272660155517.761802726601555\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 390/409 - Min Loss: 17.75 - Loss: 17.7558308148373617.75583081483736\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 391/409 - Min Loss: 17.74 - Loss: 17.74437092953828517.744370929538285\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 392/409 - Min Loss: 17.73 - Loss: 17.7357510888798217.73575108887982\n",
      "Iter: 0 - Alpha 0.05 - Batch 393/409 - Min Loss: 17.73 - Loss: 17.7374463598047517.73575108887982\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 394/409 - Min Loss: 17.72 - Loss: 17.72535492162395317.725354921623953\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 395/409 - Min Loss: 17.70 - Loss: 17.70418532875793717.704185328757937\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 396/409 - Min Loss: 17.69 - Loss: 17.6918547095019217.69185470950192\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 397/409 - Min Loss: 17.68 - Loss: 17.68129765649759517.681297656497595\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 398/409 - Min Loss: 17.67 - Loss: 17.6703899884393217.67038998843932\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 399/409 - Min Loss: 17.66 - Loss: 17.66040498943816717.660404989438167\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 400/409 - Min Loss: 17.65 - Loss: 17.65217575600840817.652175756008408\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 401/409 - Min Loss: 17.63 - Loss: 17.63685971305554317.636859713055543\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 402/409 - Min Loss: 17.62 - Loss: 17.62777081894346417.627770818943464\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 403/409 - Min Loss: 17.61 - Loss: 17.61510310531016517.615103105310165\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 404/409 - Min Loss: 17.60 - Loss: 17.60194653636189717.601946536361897\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 405/409 - Min Loss: 17.58 - Loss: 17.589112866782917.5891128667829\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 406/409 - Min Loss: 17.57 - Loss: 17.57610428471171517.576104284711715\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 407/409 - Min Loss: 17.56 - Loss: 17.56229780591179317.562297805911793\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 408/409 - Min Loss: 17.55 - Loss: 17.55024809951571317.550248099515713\n",
      "\n",
      "Iter: 0 - Alpha 0.05 - Batch 409/409 - Min Loss: 17.53 - Loss: 17.53276947442113\n"
     ]
    }
   ],
   "source": [
    "train(1) # для скорости, обучать надо гораздо больше, итераций 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "‘I wher wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout therer wherer fout \n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=500, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Предложите свои варианты решения проблемы исчезающего градиента в RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опять же основываясь на книге - затухающий градиент возникает из-за особенностей производной функции активации. К примеру функция сигмоиды имеет плоские хвосты с маленькой производной, и при обратном распространении ошибки, если возникает значение, близкое к 0 или 1, мы получим маленькое значение градиента, и с каждым слоем, в каждым умножением на производную, градиент будет только уменьшаться. Решение сразу видится такое - использовать другую функцию активации, без таких хвостов с малой производной (к примеру ReLu), но тогда может возникнуть другая проблема - взрывной рост градиента (умножение по нескольким слоям производных, каждая из которых >1, будет с каждым разом увеличивать градиент). Взрывной рост тогда можно в ручную ограничить и просто дольше обучать. Можно, наверное, ещё попробовать чередовать по слоям функции активации.\n",
    "Другое решение - поменять архитектуру чтобы было меньше умножений производных. К примеру, уменьшить число слоёв, либо сделать так, чтобы не пришлось умножать на производную функции активации. Как я онимаю, подход LSTM как раз в этом и заключается - мы добавляем гейты, которые комбинированно влияют на входной сигнал (добавляют новые пути распространения сигнала по разным правилам), и обратное распространение изменяется от простого вычисления производных и умножения матриц на более сложную, зависящую и от сигнала, и от предыдущего какого-то внутреннего состояния ячейки. Комбинация нескольких гейтов приводит к тому, что в обратном распространении сигнал не будет затухать хотя бы в одном гейте, а значит и градиент, пройдя через LSTM ячейку, не будет затухать."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
