{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучите нейронную сеть любой архитектуры которой не было на курсе, либо обучите нейронную сеть разобранной архитектуры, но на том датасете, которого не было на уроках. Сделайте анализ, того, что вам помогло в улучшения работы нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я использовал нашу модель для генерации текстов с минимальными изменениями (я использовал LSTM) и взял в качестве датасета небольшой отрывок из Шекспира (отсюда http://karpathy.github.io/2015/05/21/rnn-effectiveness/).<br>\n",
    "Параметров не так много, но я в итоге пришёл к тому, что сильно увеличил размер скрытого слоя в LSTM ячейке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Activation, LSTM\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "with open(\"shakespear.txt\", 'r', encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "vocab = list(set(raw))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = {}\n",
    "for i, w in enumerate(vocab):\n",
    "    w2i[w] = i\n",
    "text_inds = np.array(list(map(lambda x: w2i[x], raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "\n",
    "input_chars, label_chars = [], []\n",
    "for i in range(0, len(raw) - SEQLEN, STEP):\n",
    "    input_chars.append(raw[i: i + SEQLEN])\n",
    "    label_chars.append(raw[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_chars), SEQLEN, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), vocab_size), dtype=np.bool)\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, w2i[ch]] = 1\n",
    "    y[i, w2i[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 5\n",
    "NUM_PREDS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(\n",
    "        HIDDEN_SIZE,\n",
    "        return_sequences=False,\n",
    "        input_shape=(SEQLEN, vocab_size),\n",
    "        unroll=True\n",
    "    )\n",
    ")\n",
    "model.add(Dense(vocab_size))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполнение серий тренировочных и демонстрационных итераций \n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "\n",
    "    # для каждой итерации запуск передачи данных в модель \n",
    "    print(\"=\" * 50)\n",
    "    print(\"Итерация #: %d\" % (iteration))\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "\n",
    "    # Select a random example input sequence.\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "\n",
    "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
    "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
    "    print(\"Генерация из посева: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "        # здесь one-hot encoding.\n",
    "        X_test = np.zeros((1, SEQLEN, vocab_size))\n",
    "        for j, ch in enumerate(test_chars):\n",
    "            X_test[0, j, w2i[ch]] = 1\n",
    "\n",
    "        # осуществление предсказания с помощью текущей модели.\n",
    "        pred = model.predict(X_test, verbose=0)[0]\n",
    "        y_pred = vocab[np.argmax(pred)]\n",
    "\n",
    "        # вывод предсказания добавленного к тестовому примеру \n",
    "        print(y_pred, end=\"\")\n",
    "\n",
    "        # инкрементация тестового примера содержащего предсказание\n",
    "        test_chars = test_chars[1:] + y_pred\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На гитхабе нельзя свернуть результат ячейки, поэтому я вынес в отдельный файл train_output.txt ход обучения, а тут продублирую предсказание на последней эпохе последней итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nd.\n",
      "\n",
      "MARK ANTONY:\n",
      "Thou dost digest you! If accident doth seen a seat\n",
      ":s not pray their patience.'\n",
      "\n",
      "K"
     ]
    }
   ],
   "source": [
    "test_chars = \"m my husba\"\n",
    "for i in range(NUM_PREDS_PER_EPOCH):\n",
    "\n",
    "    # здесь one-hot encoding.\n",
    "    X_test = np.zeros((1, SEQLEN, vocab_size))\n",
    "    for j, ch in enumerate(test_chars):\n",
    "        X_test[0, j, w2i[ch]] = 1\n",
    "\n",
    "    # осуществление предсказания с помощью текущей модели.\n",
    "    pred = model.predict(X_test, verbose=0)[0]\n",
    "    y_pred = vocab[np.argmax(pred)]\n",
    "\n",
    "    # вывод предсказания добавленного к тестовому примеру \n",
    "    print(y_pred, end=\"\")\n",
    "\n",
    "    # инкрементация тестового примера содержащего предсказание\n",
    "    test_chars = test_chars[1:] + y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь я взял первые 10 символов из файла, который я использовал для обучения, и на их основании предсказал 1000 символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " contempt,\n",
      "Branches: to thy respect have not said here I came Butt\n",
      "With such an anointed two wisdoms, the day\n",
      "Must stay and be so hot all ad is death forbid, and thought\n",
      "The bright buds of mine own.\n",
      "\n",
      "BIONDELLO:\n",
      "Marry, there was savage.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I'll poison friendship. I hope I was a truft or dost\n",
      "have made him,\n",
      "Good Thomas.\n",
      "\n",
      "ALBANY:\n",
      "A plague of self-sweet queen as the king\n",
      "May let her, wished with my words,\n",
      "And he that keeps some suitors in the cases of the way.\n",
      "\n",
      "SICINIUS:\n",
      "We will.\n",
      "\n",
      "SALARINO:\n",
      "O, move me, how haps would not lurk to the bell!\n",
      "'Nobles. Out, three!' Omnon. Hath he not\n",
      " ended her huge envy.\n",
      "\n",
      "VENTIDIUS:\n",
      "Nor he, my lord.\n",
      "\n",
      "TRINCULO:\n",
      "My lord, it were two bodies, to discharge itself in't with my words,\n",
      "And he that keeps some suitors in the cases of the way.\n",
      "\n",
      "SICINIUS:\n",
      "We will.\n",
      "\n",
      "SALARINO:\n",
      "O, move me, how haps would not lurk to the bell!\n",
      "'Nobles. Out, three!' Omnon. Hath he not\n",
      " ended her huge envy.\n",
      "\n",
      "VENTIDIUS:\n",
      "Nor he, my lord.\n",
      "\n",
      "TRINCULO:\n",
      "My lord, it were two bodies, to disc"
     ]
    }
   ],
   "source": [
    "test_chars = \"That, poor\"\n",
    "for i in range(1000):\n",
    "\n",
    "    # здесь one-hot encoding.\n",
    "    X_test = np.zeros((1, SEQLEN, vocab_size))\n",
    "    for j, ch in enumerate(test_chars):\n",
    "        X_test[0, j, w2i[ch]] = 1\n",
    "\n",
    "    # осуществление предсказания с помощью текущей модели.\n",
    "    pred = model.predict(X_test, verbose=0)[0]\n",
    "    y_pred = vocab[np.argmax(pred)]\n",
    "\n",
    "    # вывод предсказания добавленного к тестовому примеру \n",
    "    print(y_pred, end=\"\")\n",
    "\n",
    "    # инкрементация тестового примера содержащего предсказание\n",
    "    test_chars = test_chars[1:] + y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом не очень осмысленно и текст является перемешением словосочетаний из оригинального текста, но всё равно прикольно, что он запомнил фразы, и вдвойне прикольно, что он даже структуру шекспировских сонетов сохраняет!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделайте краткий обзор какой-нибудь научной работы посвященной тому или иному алгоритму нейронных сетей, который не рассматривался на курсе. Проведите анализ: Чем отличается выбранная вами на рассмотрение архитектура нейронной сети от других архитектур? В чем плюсы и минусы данной архитектуры? Какие могут возникнуть трудности при применении данной архитектуры на практике? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я стал разбирать статью о BERT (https://arxiv.org/abs/1810.04805v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) - это модель, построенная на трансформерах, и предобучена на задачах, связанных с учёто всего контекста (и слева, и справа).<br><br>\n",
    "Сразу небольшое отступление от статьи, что бы разобраться в терминологии.<br>\n",
    "Чтобы можно было использовать машинное обучение в задачах, связанных с текстом, мы должны каким-то образом перевести текст и последовательность чисел. Самое простое - кажому слову из текста сопоставить конкретное число - индекс, тогда из текста полуич словарь связей конкретных слов и их индексов и массив чисел, который представляет из себя первоначальный текст, где все слова заменены в соответсвии с этим словарём. С таким уже можно работать, но мы знаем, что, к примеру, слова \"стул\" и \"стулья\" - это одно существительно в разных формах. Нельзя ли как-то указать компьютеру на эту особенности языка? Решение этой задачи в том, чтобы сопоставлять каждому слову не одно число, а целый вектор, да таким образом, чтобы у похожих слов вектора были близки друг к другу. Эта задача называется эмбедингом. Следующий шаг - представить целое предложение, состоящее из многих слов, в виде матрицы, которая будет учитывать отношения между словами в предложении. Эту задачу решают т.н. Энкодеры в составе моделей глубокого обучения. Есть разные архитектуры энкодеров, на основе RNN, CNN, а в трансформере от Open AI (на нем основан BERT) используется т.н. self-attention механизм. Их энкодер состоит из 2х слоёв и в первом слое (который и есть self-attention) вычисляется отношение текущего кодируемого слова ко всем остальным в предложении. Считается, что этот слой позволяет оценить похожесть слов между собой. Такая модель может быть расширена до т.н. multi-head attention - когда параллельно используется много слоёв self-attention, каждый из которых обучается воспринимать слова на основании разных аспектов. Про декодер в трансформере говорить не будем - он нам и не нужен. Если сделать хороший энкодер, обучить его на большом датасете, то затем его можно будет использовать в разных задачах и создавать и обучать придётся только часть декодера.<br><br>\n",
    "Теперь вернёмся к статье. BERT - это модель, состоящая из нескольких энкодеров из трансформера (размерность выхода из энкодера равна размерности входа, поэтому их можно стакать много-много раз), которую предобучили на большом датасете для 2х задач NLP (об этом ниже) и выложили в общий доступ. Авторы презентуют 2 модели, BERTbase и BERTlarge, первая состоит из 6ти блоков-энкодеров, вторая из 12ти, некоторые другие параметры так же у этих моделей различаются, что в итоге приводит к 110 миллионам параметров у BERTbase и около 340 миллионов параметров у BERTlarge. Отмечу, что по числу параметров BERTbase сравним с OpenAI GPT, с которым авторы сравнивают свои результаты.<br>\n",
    "Обучение проводилось для двух задач NLP. Однонаправленные модели используют половину контекста для предсказания слова. К примеру, используют первые N слов предложения для предсказания N+1. Создатели BERT изменили эту задачу, чтобы сеть использовала весь контекст. В тексте случайным образом заменялись слова на маску и модель училась предсказывать, какие слова должны быть на месте масок. Таким образом, модель имела в распоряжении весь текст сразу, т.е. контекст учитывался истинно полностью (что отличает BERT от другой модели ELMo, которая по сути состоит из 2х частей, одна предсказывает слово по контексту слева, другая по контексту справа, а затем результат конкатенируется).<br>\n",
    "Вторая задача - предсказание связи текущего предложения со следующим. Для этого был набран набор пар предложений, половина этих пар состояла из предложений, которые следовали подряд в тексте, а вторая половина, где второе предложение было выбрано случайно из текста. Обучение для такой задачи позволяет использовать BERT для задач по типу генерации ответов на вопросы.\n",
    "Чтобы использовать BERT, достаточно добавить выходной слой, который будет декодировать выход BERT в тот вид, который нужен для конкретной задачи, и обучать только этот слой. В итоге авторы получили результаты для большого числа стандартных задач NLP, которые сильно опережали предыдущих лидеров.<br><br>\n",
    "Добавлю от себя интересный факт - вскоре после выхода BERT состоялся релиз второй версии GPT (OpenAI GPT-2) с рекордным числом параметров 1.5 миллиардов (т.е. где-то в 4.5 раз больше, чем у BERTlarge). По сути эта модель является генератором текста и она оказалось настолько крута, что авторы не стали выкладывать её в свободный доступ опасаясь того, что она будет использоваться для генерации фейковых новостей."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
