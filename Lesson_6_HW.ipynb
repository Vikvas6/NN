{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краткое описание работы\n",
    "Я подумал - наверное же можно объединить все эти 3 упраженения в одно - написать свою реализацию U-Net, которую обучить на другом датасете - а именно на Carvana. <br>\n",
    "Воодушевлённый такой идеей я начал писать, в каждой проблемой всё больше и больше погружаясь в дебри гугла, в итоге я дошёл до научных статей, где был представлен U-Net (правда почти сразу после этого я нашёл статью, в которой описывается по шагам реализация U-Net (ссылка ниже), поэтому исходные статьи я не читал), и дошёл до репозитория участника этого соревнования, сделал свою реализацию и поставил обучаться. <br>\n",
    "Сначала я получал ошибки, что на GPU не хватает памяти, затем, позакрывав всё лишнее и изменив параметры обучения (точнее - уменьшил Batch Size, чтобы за раз меньше картинок грузить) я запустил снова, и компьютер начал долго думать. <br>\n",
    "Через где-то 20-30 минут температура от компа уже несло жаром, что мне конкретно напугало, но программа просмотра температуры говорила, что на видеокарте всего 70 градусов. Надо отметить, что видеокарта у меня 1060 Gaming с 6 Гб видеопамяти, так что, видимо, моя реализация оочень плохая в плане оптимизации. <br>\n",
    "В итоге я прогнал одну эпоху и получил accuracy на валидационной выборке 0.8183, что, мне кажется, неплохо. Посмотрев датасеты учасников соревнования я увидел, что могут и по 100 эпох обучать.<br>\n",
    "Кстати, можно будет в итоговом проекте с текстом работать? Мне это ближе, я решал задачу генерации сонетов, как у Шекспира, вот я чем-то подобным хотел бы заняться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "<ol>\n",
    "    <li>Попробуйте обучить нейронную сеть U-Net на любом другом датасете. \n",
    "        Опишите в комментарии к уроку - какой результата вы добились от нейросети? Что помогло вам улучшить ее точность?\n",
    "    </li>\n",
    "    <li>*Попробуйте свои силы в задаче Carvana на Kaggle - https://www.kaggle.com/c/carvana-image-masking-challenge/overview</li>\n",
    "    <li>*Сделайте свою реализацию U-Net на TensorFlow</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я заручился поддержкой интернета, и нашёл статью с хорошим (на мой взгляд) описанием реализации UNet. <br>\n",
    "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5  <br>\n",
    "Я так понимаю, это каноничная реализация архитектуры (просто по картинке), поэтому особо тут ничего не сделаешь, чтобы это не выглядело, как будто я списал. Однако мне не понравилось название переменных, я их ихменил. Слоя переименовал в соответствии с картинкой - у нас есть как бы 5 слоёв сверху вниз, в каждом из которых есть input и output части, содержащие по несколько свёрточных слоёв, переход между слоями происходит с помощью пулинга и обратной свёрткой. Вот эту структуру я постарался лучше отразить в названиях переменных. Как и в оригинальной работе, макс пулинг я отнёс к предыдущему слою, а обратную свёртку - к следующему слою.<br>\n",
    "Так же я поменял паддинг, т.к. на картинке размерность уменьшается.<br>\n",
    "Не понял последний слой - во всех примерах, что я нашёл, используется свёртка с одним фильтром, но на картинке стоит обозначение, как будто используется 2. Полагаю, что это обозначние, что на выходе мы имеем 2 класса, и выходная матрица одна и содержит 1 и 0 принадлежит пиксель объекту или нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертация изображений т.к. opencv не работает с gif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for file in glob.glob(\"carvana-image-masking-challenge/train_masks/*.gif\"):\n",
    "        img = Image.open(file)\n",
    "        img.save(file[:-3]+\".png\",'png', optimize=True, quality=95) # В названии ошибся, получилось типа 12345_01..png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Cropping2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение каноничной U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(start_filters=64, debug=False):\n",
    "    input_layer = Input(shape=(572, 572, 3))\n",
    "    \n",
    "    l1_conv1 = Conv2D(start_filters*1,(3,3),activation=\"relu\", padding=\"valid\")(input_layer)\n",
    "    l1_conv2 = Conv2D(start_filters*1,(3,3),activation=\"relu\", padding=\"valid\")(l1_conv1)\n",
    "    l1_pool = MaxPooling2D((2, 2))(l1_conv2)\n",
    "\n",
    "    l2_conv1 = Conv2D(start_filters*2,(3,3),activation=\"relu\", padding=\"valid\")(l1_pool)\n",
    "    l2_conv2 = Conv2D(start_filters*2,(3,3),activation=\"relu\", padding=\"valid\")(l2_conv1)\n",
    "    l2_pool = MaxPooling2D((2, 2))(l2_conv2)\n",
    "\n",
    "    l3_conv1 = Conv2D(start_filters*4,(3,3),activation=\"relu\", padding=\"valid\")(l2_pool)\n",
    "    l3_conv2 = Conv2D(start_filters*4,(3,3),activation=\"relu\", padding=\"valid\")(l3_conv1)\n",
    "    l3_pool = MaxPooling2D((2, 2))(l3_conv2)\n",
    "\n",
    "    l4_conv1 = Conv2D(start_filters*8,(3,3),activation=\"relu\", padding=\"valid\")(l3_pool)\n",
    "    l4_conv2 = Conv2D(start_filters*8,(3,3),activation=\"relu\", padding=\"valid\")(l4_conv1)\n",
    "    l4_pool = MaxPooling2D((2, 2))(l4_conv2)\n",
    "\n",
    "    l5_conv1 = Conv2D(start_filters*16,(3,3),activation=\"relu\", padding=\"valid\")(l4_pool)\n",
    "    l5_conv2 = Conv2D(start_filters*16,(3,3),activation=\"relu\", padding=\"valid\")(l5_conv1)\n",
    "\n",
    "    l4_deconv = Conv2DTranspose(start_filters * 8, (3, 3), strides=(2, 2), padding=\"same\")(l5_conv2)\n",
    "    l4_conv2_crop = Cropping2D(cropping=(4, 4))(l4_conv2)\n",
    "   \n",
    "    l4_concat = concatenate([l4_deconv, l4_conv2_crop])\n",
    "    l4_convu1 = Conv2D(start_filters*8,(3,3),activation=\"relu\", padding=\"valid\")(l4_concat)\n",
    "    l4_convu2 = Conv2D(start_filters*8,(3,3),activation=\"relu\", padding=\"valid\")(l4_convu1)\n",
    "\n",
    "    l3_deconv = Conv2DTranspose(start_filters*4,(3,3),strides=(2, 2), padding=\"same\")(l4_convu2)\n",
    "    l3_conv2_crop = Cropping2D(cropping=(16, 16))(l3_conv2)\n",
    "   \n",
    "    l3_concat = concatenate([l3_deconv, l3_conv2_crop])\n",
    "    l3_convu1 = Conv2D(start_filters*4,(3,3),activation=\"relu\", padding=\"valid\")(l3_concat)\n",
    "    l3_convu2 = Conv2D(start_filters*4,(3,3),activation=\"relu\", padding=\"valid\")(l3_convu1)\n",
    "\n",
    "    l2_deconv = Conv2DTranspose(start_filters*2,(3,3),strides=(2, 2), padding=\"same\")(l3_convu2)\n",
    "    l2_conv2_crop = Cropping2D(cropping=(40, 40))(l2_conv2)\n",
    "   \n",
    "    l2_concat = concatenate([l2_deconv, l2_conv2_crop])\n",
    "    l2_convu1 = Conv2D(start_filters*2,(3,3),activation=\"relu\", padding=\"valid\")(l2_concat)\n",
    "    l2_convu2 = Conv2D(start_filters*2,(3,3),activation=\"relu\", padding=\"valid\")(l2_convu1)\n",
    "\n",
    "    l1_deconv = Conv2DTranspose(start_filters*1,(3,3),strides=(2, 2), padding=\"same\")(l2_convu2)\n",
    "    l1_conv2_crop = Cropping2D(cropping=(88, 88))(l1_conv2)\n",
    "   \n",
    "    l1_concat = concatenate([l1_deconv, l1_conv2_crop])\n",
    "    l1_convu1 = Conv2D(start_filters*1,(3,3),activation=\"relu\", padding=\"valid\")(l1_concat)\n",
    "    l1_convu2 = Conv2D(start_filters*1,(3,3),activation=\"relu\", padding=\"valid\")(l1_convu1)\n",
    "\n",
    "    output_layer = Conv2D(1,(1,1), padding=\"same\", activation=\"sigmoid\")(l1_convu2)\n",
    "    \n",
    "    if debug:    \n",
    "        print(f\"Shape of l1_conv1 = {l1_conv1.shape}\")\n",
    "        print(f\"Shape of l1_conv2 = {l1_conv2.shape}\")\n",
    "        print(f\"Shape of l1_pool = {l1_pool.shape}\")\n",
    "        print(f\"Shape of l2_conv1 = {l2_conv1.shape}\")\n",
    "        print(f\"Shape of l2_conv2 = {l2_conv2.shape}\")\n",
    "        print(f\"Shape of l2_pool = {l2_pool.shape}\")\n",
    "        print(f\"Shape of l3_conv1 = {l3_conv1.shape}\")\n",
    "        print(f\"Shape of l3_conv2 = {l3_conv2.shape}\")\n",
    "        print(f\"Shape of l3_pool = {l3_pool.shape}\")\n",
    "        print(f\"Shape of l4_conv1 = {l4_conv1.shape}\")\n",
    "        print(f\"Shape of l4_conv2 = {l4_conv2.shape}\")\n",
    "        print(f\"Shape of l4_pool = {l4_pool.shape}\")\n",
    "        print(f\"Shape of l5_conv1 = {l5_conv1.shape}\")\n",
    "        print(f\"Shape of l5_conv2 = {l5_conv2.shape}\")\n",
    "        print(f\"Shape of l4_deconv = {l4_deconv.shape}\")\n",
    "        print(f\"Shape of l4_conv2 = {l4_conv2_crop.shape}\")\n",
    "        print(f\"Shape of l4_concat = {l4_concat.shape}\")\n",
    "        print(f\"Shape of l4_convu1 = {l4_convu1.shape}\")\n",
    "        print(f\"Shape of l4_convu2 = {l4_convu2.shape}\")\n",
    "        print(f\"Shape of l3_deconv = {l3_deconv.shape}\")\n",
    "        print(f\"Shape of l3_conv2 = {l3_conv2_crop.shape}\")\n",
    "        print(f\"Shape of l3_concat = {l3_concat.shape}\")\n",
    "        print(f\"Shape of l3_convu1 = {l3_convu1.shape}\")\n",
    "        print(f\"Shape of l3_convu2 = {l3_convu2.shape}\")\n",
    "        print(f\"Shape of l2_deconv = {l2_deconv.shape}\")\n",
    "        print(f\"Shape of l2_conv2 = {l2_conv2_crop.shape}\")\n",
    "        print(f\"Shape of l2_concat = {l2_concat.shape}\")\n",
    "        print(f\"Shape of l2_convu1 = {l2_convu1.shape}\")\n",
    "        print(f\"Shape of l2_convu2 = {l2_convu2.shape}\")\n",
    "        print(f\"Shape of l1_deconv = {l1_deconv.shape}\")\n",
    "        print(f\"Shape of l1_conv2 = {l1_conv2_crop.shape}\")\n",
    "        print(f\"Shape of l1_concat = {l1_concat.shape}\")\n",
    "        print(f\"Shape of l1_convu1 = {l1_convu1.shape}\")\n",
    "        print(f\"Shape of l1_convu2 = {l1_convu2.shape}\")\n",
    "        print(f\"Shape of output_layer = {output_layer.shape}\")\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of l1_conv1 = (None, 570, 570, 64)\n",
      "Shape of l1_conv2 = (None, 568, 568, 64)\n",
      "Shape of l1_pool = (None, 284, 284, 64)\n",
      "Shape of l2_conv1 = (None, 282, 282, 128)\n",
      "Shape of l2_conv2 = (None, 280, 280, 128)\n",
      "Shape of l2_pool = (None, 140, 140, 128)\n",
      "Shape of l3_conv1 = (None, 138, 138, 256)\n",
      "Shape of l3_conv2 = (None, 136, 136, 256)\n",
      "Shape of l3_pool = (None, 68, 68, 256)\n",
      "Shape of l4_conv1 = (None, 66, 66, 512)\n",
      "Shape of l4_conv2 = (None, 64, 64, 512)\n",
      "Shape of l4_pool = (None, 32, 32, 512)\n",
      "Shape of l5_conv1 = (None, 30, 30, 1024)\n",
      "Shape of l5_conv2 = (None, 28, 28, 1024)\n",
      "Shape of l4_deconv = (None, 56, 56, 512)\n",
      "Shape of l4_conv2 = (None, 56, 56, 512)\n",
      "Shape of l4_concat = (None, 56, 56, 1024)\n",
      "Shape of l4_convu1 = (None, 54, 54, 512)\n",
      "Shape of l4_convu2 = (None, 52, 52, 512)\n",
      "Shape of l3_deconv = (None, 104, 104, 256)\n",
      "Shape of l3_conv2 = (None, 104, 104, 256)\n",
      "Shape of l3_concat = (None, 104, 104, 512)\n",
      "Shape of l3_convu1 = (None, 102, 102, 256)\n",
      "Shape of l3_convu2 = (None, 100, 100, 256)\n",
      "Shape of l2_deconv = (None, 200, 200, 128)\n",
      "Shape of l2_conv2 = (None, 200, 200, 128)\n",
      "Shape of l2_concat = (None, 200, 200, 256)\n",
      "Shape of l2_convu1 = (None, 198, 198, 128)\n",
      "Shape of l2_convu2 = (None, 196, 196, 128)\n",
      "Shape of l1_deconv = (None, 392, 392, 64)\n",
      "Shape of l1_conv2 = (None, 392, 392, 64)\n",
      "Shape of l1_concat = (None, 392, 392, 128)\n",
      "Shape of l1_convu1 = (None, 390, 390, 64)\n",
      "Shape of l1_convu2 = (None, 388, 388, 64)\n",
      "Shape of output_layer = (None, 388, 388, 1)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 4070 samples\n",
      "Validating on 1018 samples\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_size = 572\n",
    "output_size = 388\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 2\n",
    "\n",
    "orig_width = 1918\n",
    "orig_height = 1280\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "df_train = pd.read_csv('carvana-image-masking-challenge/train_masks.csv')\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "ids_train_split, ids_valid_split = train_test_split(ids_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training on {len(ids_train_split)} samples')\n",
    "print(f'Validating on {len(ids_valid_split)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(ids_train_split), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(ids_train_split))\n",
    "            ids_train_batch = ids_train_split[start:end]\n",
    "            for id in ids_train_batch.values:\n",
    "                img = cv2.imread(f'carvana-image-masking-challenge/train/{id}.jpg')\n",
    "                img = cv2.resize(img, (input_size, input_size))\n",
    "                mask = cv2.imread(f'carvana-image-masking-challenge/train_masks/{id}_mask..png', cv2.IMREAD_GRAYSCALE)\n",
    "                mask = cv2.resize(mask, (output_size, output_size))\n",
    "                mask = np.expand_dims(mask, axis=2)\n",
    "                x_batch.append(img)\n",
    "                y_batch.append(mask)\n",
    "            x_batch = np.array(x_batch, np.float32) / 255\n",
    "            y_batch = np.array(y_batch, np.float32) / 255\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def valid_generator():\n",
    "    while True:\n",
    "        for start in range(0, len(ids_valid_split), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(ids_valid_split))\n",
    "            ids_valid_batch = ids_valid_split[start:end]\n",
    "            for id in ids_valid_batch.values:\n",
    "                img = cv2.imread(f'carvana-image-masking-challenge/train/{id}.jpg')\n",
    "                img = cv2.resize(img, (input_size, input_size))\n",
    "                mask = cv2.imread(f'carvana-image-masking-challenge/train_masks/{id}_mask..png', cv2.IMREAD_GRAYSCALE)\n",
    "                mask = cv2.resize(mask, (output_size, output_size))\n",
    "                mask = np.expand_dims(mask, axis=2)\n",
    "                x_batch.append(img)\n",
    "                y_batch.append(mask)\n",
    "            x_batch = np.array(x_batch, np.float32) / 255\n",
    "            y_batch = np.array(y_batch, np.float32) / 255\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0427 14:08:29.881245  7856 callbacks.py:1892] `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4),\n",
    "             ModelCheckpoint(monitor='val_loss',\n",
    "                             filepath='weights/best_weights.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True),\n",
    "             TensorBoard(log_dir='logs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0427 14:08:32.236544  7856 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "W0427 14:08:32.360060  7856 data_adapter.py:1091] sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 2035.0 steps, validate for 509.0 steps\n",
      "2035/2035 - 1119s - loss: 0.4445 - accuracy: 0.8037 - val_loss: 0.3887 - val_accuracy: 0.8183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4db2eef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=train_generator(),\n",
    "                    steps_per_epoch=np.ceil(float(len(ids_train_split)) / float(batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(),\n",
    "                    validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
